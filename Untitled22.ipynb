{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06cc2acb47f84537af1af28ff300314e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acd1e428f54e4e5a90325553ba72289e",
              "IPY_MODEL_8d150ac54729467c8d49187593973640",
              "IPY_MODEL_fbd43f87db09460d8cebcc0858572719"
            ],
            "layout": "IPY_MODEL_16bd0982e824453aaf9245de24214360"
          }
        },
        "acd1e428f54e4e5a90325553ba72289e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f955de3c61574e80b03514d03ced24df",
            "placeholder": "​",
            "style": "IPY_MODEL_96f84fa042254ddda3e4455ed8a22b2b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8d150ac54729467c8d49187593973640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d007d45dc30d44ce8b027524de5115fc",
            "max": 380267417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54f741d062c7430f8b06b39b2fa11b42",
            "value": 380267417
          }
        },
        "fbd43f87db09460d8cebcc0858572719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3864ed176e248038936bdb552f10793",
            "placeholder": "​",
            "style": "IPY_MODEL_198ac2c7e1de47c6a51e5618da3a6ebb",
            "value": " 380M/380M [00:01&lt;00:00, 423MB/s]"
          }
        },
        "16bd0982e824453aaf9245de24214360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f955de3c61574e80b03514d03ced24df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96f84fa042254ddda3e4455ed8a22b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d007d45dc30d44ce8b027524de5115fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54f741d062c7430f8b06b39b2fa11b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3864ed176e248038936bdb552f10793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "198ac2c7e1de47c6a51e5618da3a6ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30bb2014a5c2475ba0ad804949ab55e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4543cf139dd42d7ae32c5380ec8ffcf",
              "IPY_MODEL_e6f35bd4c2c844f58c5677074303e2a2",
              "IPY_MODEL_72448f4ec9e44a9ca620ac93c3e5c722"
            ],
            "layout": "IPY_MODEL_7d58aeee224049abb6cd34a0171c8cf5"
          }
        },
        "b4543cf139dd42d7ae32c5380ec8ffcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b322ec1216eb43cb8ef2df9712caf35d",
            "placeholder": "​",
            "style": "IPY_MODEL_e8b9a1772cf84f8d962a356dfa32fb15",
            "value": "model.safetensors: 100%"
          }
        },
        "e6f35bd4c2c844f58c5677074303e2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2496bdb6f7f4612ba69cc6680ab747e",
            "max": 380204696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5715d7719c14975b2a6d5eb4211a7ef",
            "value": 380204696
          }
        },
        "72448f4ec9e44a9ca620ac93c3e5c722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86d88460d2944f28be64315644ff3bac",
            "placeholder": "​",
            "style": "IPY_MODEL_0007f2d7befa4c5197c3aea75f3dab6f",
            "value": " 380M/380M [00:00&lt;00:00, 393MB/s]"
          }
        },
        "7d58aeee224049abb6cd34a0171c8cf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b322ec1216eb43cb8ef2df9712caf35d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8b9a1772cf84f8d962a356dfa32fb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2496bdb6f7f4612ba69cc6680ab747e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5715d7719c14975b2a6d5eb4211a7ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86d88460d2944f28be64315644ff3bac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0007f2d7befa4c5197c3aea75f3dab6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ash-0154/A-Machine-Learning-Approach-to-Predicting-Mental-Health-State-from-Smartwatches/blob/main/Untitled22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision torchaudio transformers datasets kagglehub librosa soundfile scikit-learn pandas numpy matplotlib seaborn psutil tqdm"
      ],
      "metadata": {
        "id": "CGWleKx0cosx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub --quiet\n",
        "import kagglehub\n",
        "crema_d_path = kagglehub.dataset_download(\"ejlok1/cremad\")\n",
        "ravdess_path  = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "tess_path = kagglehub.dataset_download(\"ejlok1/toronto-emotional-speech-set-tess\")\n",
        "shemo_path = kagglehub.dataset_download(\"mansourehk/shemo-persian-speech-emotion-detection-database\")\n",
        "subesco_path=kagglehub.dataset_download(\"sushmit0109/subescobangla-speech-emotion-dataset\")\n",
        "\n",
        "print(\"Path to subesco:\", subesco_path)\n",
        "print(shemo_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IgldHKad2bB",
        "outputId": "5ca8f323-fbd6-42ee-ff58-0bfd550f08b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'cremad' dataset.\n",
            "Using Colab cache for faster access to the 'ravdess-emotional-speech-audio' dataset.\n",
            "Using Colab cache for faster access to the 'toronto-emotional-speech-set-tess' dataset.\n",
            "Using Colab cache for faster access to the 'shemo-persian-speech-emotion-detection-database' dataset.\n",
            "Using Colab cache for faster access to the 'subescobangla-speech-emotion-dataset' dataset.\n",
            "Path to subesco: /kaggle/input/subescobangla-speech-emotion-dataset\n",
            "/kaggle/input/shemo-persian-speech-emotion-detection-database\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "unified_federated_emotion_recognition.py\n",
        "\n",
        "Combined federated learning system for text and speech emotion recognition.\n",
        "Supports multiple datasets across both modalities with federated averaging.\n",
        "Optimized for A100 GPU with enhanced architectures and regularization.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# PACKAGE INSTALLATION (Run this cell first in Jupyter Notebook)\n",
        "# ============================================================================\n",
        "print(\"Installing required packages...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "packages = [\n",
        "    'transformers',\n",
        "    'datasets',\n",
        "    'kagglehub',\n",
        "    'torch',\n",
        "    'torchvision',\n",
        "    'torchaudio',\n",
        "    'librosa',\n",
        "    'scikit-learn',\n",
        "    'pandas',\n",
        "    'numpy',\n",
        "    'matplotlib',\n",
        "    'seaborn',\n",
        "    'psutil',\n",
        "    'tqdm'\n",
        "]\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install all required packages\"\"\"\n",
        "    for package in packages:\n",
        "        try:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "            print(f\"  ✓ {package} installed successfully\")\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"  ✗ Failed to install {package}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error installing {package}: {e}\")\n",
        "\n",
        "# Uncomment the line below to install packages\n",
        "# install_packages()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Package installation complete!\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import random\n",
        "import hashlib\n",
        "import gc\n",
        "import psutil\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset, WeightedRandomSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, f1_score, recall_score,\n",
        "                            precision_score, confusion_matrix,\n",
        "                            precision_recall_fscore_support)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Text processing\n",
        "from transformers import (DistilBertTokenizerFast, DistilBertModel,\n",
        "                         get_linear_schedule_with_warmup)\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Audio processing\n",
        "import librosa\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "\n",
        "# Dataset loaders\n",
        "import kagglehub\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 32 if torch.cuda.is_available() else 16\n",
        "ROUNDS = 30\n",
        "LOCAL_EPOCHS = 3\n",
        "MAX_LEN = 128  # Text sequence length\n",
        "SAMPLE_LIMIT = 20000  # Text samples per dataset\n",
        "\n",
        "# Audio parameters\n",
        "SAMPLE_RATE = 16000\n",
        "TARGET_LEN = SAMPLE_RATE * 4\n",
        "CACHE_DIR = \"/tmp/emo_cache_fl\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Learning rates\n",
        "LR_TEXT_CLASSIFIER = 3e-5\n",
        "LR_TEXT_BERT = 2e-5\n",
        "LR_AUDIO_NEW = 8e-4\n",
        "LR_AUDIO_WAV2VEC = 2e-5\n",
        "\n",
        "# Regularization\n",
        "WEIGHT_DECAY = 1e-3\n",
        "GRAD_CLIP = 2.0\n",
        "LABEL_SMOOTHING = 0.1\n",
        "WARMUP_RATIO = 0.1\n",
        "WARMUP_ROUNDS = 2\n",
        "\n",
        "# Audio augmentation\n",
        "MIXUP_PROB = 0.65\n",
        "MIXUP_ALPHA = 0.45\n",
        "SPEC_AUG_MASK = 0.3\n",
        "PITCH_SHIFT_PROB = 0.35\n",
        "TIME_STRETCH_PROB = 0.35\n",
        "NOISE_PROB = 0.25\n",
        "\n",
        "# Federated learning\n",
        "FEDPROX_MU = 1.5e-3\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "CONFUSION_BOOST_FACTOR = 2.8\n",
        "\n",
        "NUM_CLASSES = 7  # Unified emotion classes\n",
        "NUM_WORKERS = 4 if torch.cuda.is_available() else 0\n",
        "\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
        "\n",
        "print(f\"Batch Size: {BATCH_SIZE}, Mixed Precision: {use_amp}\")\n",
        "\n",
        "# ============================================================================\n",
        "# UNIFIED EMOTION MAPPING\n",
        "# ============================================================================\n",
        "\n",
        "# Unified emotion classes (7 core emotions)\n",
        "UNIFIED_EMOTIONS = ['anger', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "# Text emotion mappings\n",
        "TEXT_EMOTION_MAP = {\n",
        "    'anger': 'anger', 'fear': 'fear', 'joy': 'happy',\n",
        "    'sadness': 'sad', 'love': 'happy', 'surprise': 'surprise',\n",
        "    '0': 'sad', '1': 'happy', '2': 'happy',\n",
        "    '3': 'anger', '4': 'fear', '5': 'surprise',\n",
        "    'happiness': 'happy', 'neutral': 'neutral', 'worry': 'fear',\n",
        "    'hate': 'anger', 'boredom': 'neutral', 'enthusiasm': 'happy',\n",
        "    'fun': 'happy', 'relief': 'happy', 'empty': 'sad',\n",
        "    'disgust': 'disgust', 'shame': 'sad', 'guilt': 'sad',\n",
        "    'confusion': 'neutral', 'desire': 'happy', 'sarcasm': 'neutral'\n",
        "}\n",
        "\n",
        "# Audio emotion mappings\n",
        "AUDIO_LABEL_MAP = {\n",
        "    \"RAVDESS\": {\"01\":\"neutral\",\"02\":\"neutral\",\"03\":\"happy\",\"04\":\"sad\",\n",
        "                \"05\":\"anger\",\"06\":\"fear\",\"07\":\"disgust\",\"08\":\"surprise\"},\n",
        "    \"CREMA-D\": {\"ANG\":\"anger\",\"DIS\":\"disgust\",\"FEA\":\"fear\",\"HAP\":\"happy\",\n",
        "                \"NEU\":\"neutral\",\"SAD\":\"sad\"},\n",
        "    \"TESS\": {\"angry\":\"anger\",\"disgust\":\"disgust\",\"fear\":\"fear\",\"happy\":\"happy\",\n",
        "             \"neutral\":\"neutral\",\"pleasant surprise\":\"surprise\",\"ps\":\"surprise\",\"sad\":\"sad\"},\n",
        "    \"ShEMO\": {\"A\":\"anger\",\"H\":\"happy\",\"S\":\"sad\",\"N\":\"neutral\",\"F\":\"fear\",\"W\":\"surprise\"},\n",
        "    \"SUBESCO\": {\"angry\":\"anger\",\"anger\":\"anger\",\"happy\":\"happy\",\"happiness\":\"happy\",\n",
        "                \"sad\":\"sad\",\"sadness\":\"sad\",\"neutral\":\"neutral\",\"surprise\":\"surprise\",\n",
        "                \"surprised\":\"surprise\",\"fear\":\"fear\",\"fearful\":\"fear\"}\n",
        "}\n",
        "\n",
        "GENERIC_TOKENS = {\"angry\":\"anger\",\"anger\":\"anger\",\"happy\":\"happy\",\"joy\":\"happy\",\n",
        "                  \"sad\":\"sad\",\"sadness\":\"sad\",\"neutral\":\"neutral\",\"surprise\":\"surprise\",\n",
        "                  \"fear\":\"fear\",\"disgust\":\"disgust\"}\n",
        "\n",
        "def normalize_text_label(label):\n",
        "    \"\"\"Normalize text emotion labels to unified format\"\"\"\n",
        "    label_str = str(label).strip().lower()\n",
        "    if label_str.replace('.', '').isdigit():\n",
        "        label_str = str(int(float(label_str)))\n",
        "    return TEXT_EMOTION_MAP.get(label_str, label_str)\n",
        "\n",
        "def normalize_audio_label(token, dataset_name=None):\n",
        "    \"\"\"Normalize audio emotion labels to unified format\"\"\"\n",
        "    if token is None:\n",
        "        return None\n",
        "    t = token.lower()\n",
        "\n",
        "    if dataset_name and dataset_name in AUDIO_LABEL_MAP:\n",
        "        dmap = AUDIO_LABEL_MAP[dataset_name]\n",
        "        if token in dmap:\n",
        "            return dmap[token]\n",
        "        if t in dmap:\n",
        "            return dmap[t]\n",
        "        if dataset_name == \"RAVDESS\" and token.isdigit():\n",
        "            return dmap.get(token)\n",
        "\n",
        "    for k, v in GENERIC_TOKENS.items():\n",
        "        if k in t:\n",
        "            return v\n",
        "    return None\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def cleanup_model(model):\n",
        "    \"\"\"Clean up model from memory\"\"\"\n",
        "    if hasattr(model, 'cpu'):\n",
        "        try:\n",
        "            model.cpu()\n",
        "        except:\n",
        "            pass\n",
        "    del model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def aggressive_cleanup():\n",
        "    \"\"\"Aggressive memory cleanup\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def print_memory_usage():\n",
        "    \"\"\"Print current memory usage\"\"\"\n",
        "    ram = psutil.virtual_memory().percent\n",
        "    vram = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
        "    print(f\"   RAM: {ram:.1f}% | VRAM: {vram:.2f}GB\")\n",
        "\n",
        "def compute_metrics(trues, preds):\n",
        "    \"\"\"Compute evaluation metrics\"\"\"\n",
        "    if len(trues) == 0:\n",
        "        return 0.0, 0.0, 0.0, 0.0\n",
        "    acc = accuracy_score(trues, preds)\n",
        "    f1 = f1_score(trues, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(trues, preds, average='weighted', zero_division=0)\n",
        "    precision = precision_score(trues, preds, average='weighted', zero_division=0)\n",
        "    return acc, f1, recall, precision\n",
        "\n",
        "def get_lr_multiplier(round_num, warmup_rounds=WARMUP_ROUNDS, total_rounds=ROUNDS):\n",
        "    \"\"\"Warmup + Cosine annealing schedule\"\"\"\n",
        "    if round_num < warmup_rounds:\n",
        "        return (round_num + 1) / warmup_rounds\n",
        "    else:\n",
        "        progress = (round_num - warmup_rounds) / (total_rounds - warmup_rounds)\n",
        "        return 0.5 * (1 + np.cos(np.pi * progress))\n",
        "\n",
        "def mixup_batch_audio(batch, alpha=0.5):\n",
        "    \"\"\"Apply mixup augmentation to audio batch\"\"\"\n",
        "    wav, mel, wav2vec_emb, labels = batch\n",
        "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n",
        "    n = wav.size(0)\n",
        "    idx = torch.randperm(n)\n",
        "\n",
        "    wav2 = wav[idx]\n",
        "    mel2 = mel[idx]\n",
        "    wav2vec2 = None if wav2vec_emb is None else wav2vec_emb[idx]\n",
        "    labels2 = labels[idx]\n",
        "\n",
        "    wav_m = lam * wav + (1 - lam) * wav2\n",
        "    mel_m = lam * mel + (1 - lam) * mel2\n",
        "    wav2vec_m = None if wav2vec_emb is None else lam * wav2vec_emb + (1 - lam) * wav2vec2\n",
        "\n",
        "    return wav_m, mel_m, wav2vec_m, labels, labels2, lam\n",
        "\n",
        "# ============================================================================\n",
        "# TEXT DATA PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING TEXT DATASETS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load HuggingFace text datasets\n",
        "try:\n",
        "    hf_clients = {\n",
        "        \"dair-ai/emotion\": load_dataset(\"dair-ai/emotion\"),\n",
        "        \"boltuix/emotions-dataset\": load_dataset(\"boltuix/emotions-dataset\"),\n",
        "        \"mteb/emotion\": load_dataset(\"mteb/emotion\"),\n",
        "        \"go_emotions\": load_dataset(\"go_emotions\")\n",
        "    }\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Some HF datasets failed to load: {e}\")\n",
        "    hf_clients = {}\n",
        "\n",
        "# Load Kaggle text dataset\n",
        "try:\n",
        "    dataset_path = kagglehub.dataset_download(\"pashupatigupta/emotion-detection-from-text\")\n",
        "    csv_files = [f for f in os.listdir(dataset_path) if f.endswith(\".csv\")]\n",
        "    if csv_files:\n",
        "        csv_path = os.path.join(dataset_path, csv_files[0])\n",
        "        df_kaggle = pd.read_csv(csv_path)\n",
        "        kaggle_client = {\"train\": df_kaggle.to_dict(\"records\")}\n",
        "        print(f\"✓ Loaded Kaggle text dataset: {len(df_kaggle)} samples\")\n",
        "    else:\n",
        "        kaggle_client = {}\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Kaggle text dataset not loaded: {e}\")\n",
        "    kaggle_client = {}\n",
        "\n",
        "text_clients_datasets = {**hf_clients, \"pashupatigupta/emotion\": kaggle_client}\n",
        "\n",
        "def prepare_text_client_data(ds, dataset_name):\n",
        "    \"\"\"Prepare text data from various dataset formats\"\"\"\n",
        "    if isinstance(ds, dict) and \"train\" in ds:\n",
        "        data_iter = ds[\"train\"]\n",
        "        sample_item = ds[\"train\"][0]\n",
        "    else:\n",
        "        return [], []\n",
        "\n",
        "    # Find text column\n",
        "    preferred_text_cols = ['text', 'sentence', 'content', 'tweet', 'utterance']\n",
        "    lower_keys = {k.lower(): k for k in sample_item.keys()}\n",
        "    text_candidates = [lower_keys[col] for col in preferred_text_cols if col in lower_keys]\n",
        "\n",
        "    if not text_candidates:\n",
        "        text_candidates = [k for k in sample_item.keys()\n",
        "                          if any(tok in k.lower() for tok in preferred_text_cols)]\n",
        "\n",
        "    text_col = text_candidates[0] if text_candidates else list(sample_item.keys())[0]\n",
        "\n",
        "    # Find label column\n",
        "    preferred_label_cols = ['label', 'emotion', 'sentiment', 'category']\n",
        "    label_candidates = [lower_keys[col] for col in preferred_label_cols if col in lower_keys]\n",
        "\n",
        "    if not label_candidates:\n",
        "        label_candidates = [k for k in sample_item.keys()\n",
        "                           if any(tok in k.lower() for tok in preferred_label_cols)]\n",
        "\n",
        "    label_col = label_candidates[0] if label_candidates else list(sample_item.keys())[1]\n",
        "\n",
        "    texts, labels = [], []\n",
        "    for i, item in enumerate(data_iter):\n",
        "        if len(texts) >= SAMPLE_LIMIT:\n",
        "            break\n",
        "\n",
        "        text = str(item[text_col]).strip()\n",
        "        if len(text) < 10:\n",
        "            continue\n",
        "\n",
        "        lbl = item[label_col]\n",
        "        if isinstance(lbl, list):\n",
        "            lbl = lbl[0]\n",
        "\n",
        "        lbl = normalize_text_label(lbl)\n",
        "        if lbl in UNIFIED_EMOTIONS:\n",
        "            texts.append(text)\n",
        "            labels.append(lbl)\n",
        "\n",
        "    print(f\"  ✓ {dataset_name}: {len(texts)} samples\")\n",
        "    return texts, labels\n",
        "\n",
        "text_client_texts, text_client_labels = {}, {}\n",
        "for name, ds in text_clients_datasets.items():\n",
        "    if ds:\n",
        "        texts, labels = prepare_text_client_data(ds, name)\n",
        "        if texts:\n",
        "            text_client_texts[name] = texts\n",
        "            text_client_labels[name] = labels\n",
        "\n",
        "print(f\"Loaded {len(text_client_texts)} text datasets\")\n",
        "\n",
        "# ============================================================================\n",
        "# AUDIO DATA PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING AUDIO DATASETS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Audio dataset paths (update these for your environment)\n",
        "AUDIO_DATASET_PATHS = {\n",
        "    \"CREMA-D\": \"/kaggle/input/cremad/AudioWAV\",\n",
        "    \"RAVDESS\": \"/kaggle/input/ravdess-emotional-speech-audio\",\n",
        "    \"TESS\": \"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data\",\n",
        "    \"ShEMO-Male\": \"/kaggle/input/shemo-persian-speech-emotion-detection-database/male\",\n",
        "    \"ShEMO-Female\": \"/kaggle/input/shemo-persian-speech-emotion-detection-database/female\",\n",
        "    \"SUBESCO\": \"/kaggle/input/subescobangla-speech-emotion-dataset/SUBESCO\"\n",
        "}\n",
        "\n",
        "def gather_ravdess(path):\n",
        "    \"\"\"Gather RAVDESS dataset files\"\"\"\n",
        "    files, labels = [], []\n",
        "    for root, _, fnames in os.walk(path):\n",
        "        for f in fnames:\n",
        "            if f.lower().endswith('.wav'):\n",
        "                parts = f.split('-')\n",
        "                if len(parts) >= 3:\n",
        "                    emo = normalize_audio_label(parts[2], \"RAVDESS\")\n",
        "                    if emo:\n",
        "                        files.append(os.path.join(root, f))\n",
        "                        labels.append(emo)\n",
        "    return files, labels\n",
        "\n",
        "def gather_cremad(path):\n",
        "    \"\"\"Gather CREMA-D dataset files\"\"\"\n",
        "    files, labels = [], []\n",
        "    for root, _, fnames in os.walk(path):\n",
        "        for f in fnames:\n",
        "            if f.lower().endswith('.wav'):\n",
        "                parts = f.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    emo = AUDIO_LABEL_MAP[\"CREMA-D\"].get(parts[2][:3].upper())\n",
        "                    if emo:\n",
        "                        files.append(os.path.join(root, f))\n",
        "                        labels.append(emo)\n",
        "    return files, labels\n",
        "\n",
        "def gather_tess(path):\n",
        "    \"\"\"Gather TESS dataset files\"\"\"\n",
        "    files, labels = [], []\n",
        "    for root, _, fnames in os.walk(path):\n",
        "        parent_emo = normalize_audio_label(os.path.basename(root).lower(), \"TESS\")\n",
        "        for f in fnames:\n",
        "            if f.lower().endswith('.wav'):\n",
        "                emo = normalize_audio_label(f.lower(), \"TESS\") or parent_emo\n",
        "                if emo:\n",
        "                    files.append(os.path.join(root, f))\n",
        "                    labels.append(emo)\n",
        "    return files, labels\n",
        "\n",
        "def gather_shemo(path):\n",
        "    \"\"\"Gather ShEMO dataset files\"\"\"\n",
        "    files, labels = [], []\n",
        "    for root, _, fnames in os.walk(path):\n",
        "        for f in fnames:\n",
        "            if f.lower().endswith('.wav') and len(f) >= 4:\n",
        "                emotion_code = f[3].upper()\n",
        "                emotion = AUDIO_LABEL_MAP[\"ShEMO\"].get(emotion_code)\n",
        "                if emotion:\n",
        "                    files.append(os.path.join(root, f))\n",
        "                    labels.append(emotion)\n",
        "    return files, labels\n",
        "\n",
        "def gather_subesco(path):\n",
        "    \"\"\"Gather SUBESCO dataset files\"\"\"\n",
        "    files, labels = [], []\n",
        "    for root, dirs, fnames in os.walk(path):\n",
        "        folder_name = os.path.basename(root).lower()\n",
        "        folder_emotion = normalize_audio_label(folder_name, \"SUBESCO\")\n",
        "\n",
        "        for f in fnames:\n",
        "            if f.lower().endswith('.wav'):\n",
        "                full_path = os.path.join(root, f)\n",
        "                fname_lower = f.lower()\n",
        "\n",
        "                file_emotion = None\n",
        "                for emo_key in AUDIO_LABEL_MAP[\"SUBESCO\"].keys():\n",
        "                    if emo_key in fname_lower:\n",
        "                        file_emotion = AUDIO_LABEL_MAP[\"SUBESCO\"][emo_key]\n",
        "                        break\n",
        "\n",
        "                emotion = file_emotion or folder_emotion\n",
        "                if emotion:\n",
        "                    files.append(full_path)\n",
        "                    labels.append(emotion)\n",
        "    return files, labels\n",
        "\n",
        "# Gather audio datasets\n",
        "audio_clients_files, audio_clients_labels = {}, {}\n",
        "shemo_files_combined, shemo_labels_combined = [], []\n",
        "\n",
        "for name, path in AUDIO_DATASET_PATHS.items():\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"  ✗ Missing: {path}\")\n",
        "        continue\n",
        "\n",
        "    if \"RAVDESS\" in name.upper():\n",
        "        f, l = gather_ravdess(path)\n",
        "    elif \"CREMA\" in name.upper():\n",
        "        f, l = gather_cremad(path)\n",
        "    elif \"TESS\" in name.upper():\n",
        "        f, l = gather_tess(path)\n",
        "    elif \"SHEMO\" in name.upper():\n",
        "        f, l = gather_shemo(path)\n",
        "        shemo_files_combined.extend(f)\n",
        "        shemo_labels_combined.extend(l)\n",
        "        print(f\"  ✓ {name}: {len(f)} files | Dist: {dict(Counter(l))}\")\n",
        "        continue\n",
        "    elif \"SUBESCO\" in name.upper():\n",
        "        f, l = gather_subesco(path)\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    if f and len(f) > 100:\n",
        "        audio_clients_files[name] = f\n",
        "        audio_clients_labels[name] = l\n",
        "        print(f\"  ✓ {name}: {len(f)} files | Dist: {dict(Counter(l))}\")\n",
        "\n",
        "# Combine ShEMO datasets\n",
        "if shemo_files_combined and len(shemo_files_combined) > 100:\n",
        "    audio_clients_files[\"ShEMO\"] = shemo_files_combined\n",
        "    audio_clients_labels[\"ShEMO\"] = shemo_labels_combined\n",
        "    print(f\"  ✓ ShEMO (Combined): {len(shemo_files_combined)} files | Dist: {dict(Counter(shemo_labels_combined))}\")\n",
        "\n",
        "print(f\"Loaded {len(audio_clients_files)} audio datasets\")\n",
        "\n",
        "# ============================================================================\n",
        "# LABEL ENCODING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENCODING LABELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Collect all labels from both modalities\n",
        "all_labels_combined = []\n",
        "all_labels_combined.extend([lbl for labs in text_client_labels.values() for lbl in labs])\n",
        "all_labels_combined.extend([lbl for labs in audio_clients_labels.values() for lbl in labs])\n",
        "\n",
        "# Filter to only include unified emotions\n",
        "all_labels_filtered = [lbl for lbl in all_labels_combined if lbl in UNIFIED_EMOTIONS]\n",
        "\n",
        "# Create label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(UNIFIED_EMOTIONS)\n",
        "NUM_CLASSES = len(label_encoder.classes_)\n",
        "\n",
        "print(f\"Unified emotion classes ({NUM_CLASSES}): {list(label_encoder.classes_)}\")\n",
        "\n",
        "# Compute class weights\n",
        "all_label_nums = label_encoder.transform(all_labels_filtered)\n",
        "class_weights = compute_class_weight('balanced', classes=np.arange(NUM_CLASSES), y=all_label_nums)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "print(f\"Class weights: {dict(zip(label_encoder.classes_, class_weights.round(3)))}\")\n",
        "\n",
        "# ============================================================================\n",
        "# TEXT MODEL ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "class TokenCNN_BiLSTM_Attention(nn.Module):\n",
        "    \"\"\"Hybrid CNN-BiLSTM-Attention module for text\"\"\"\n",
        "    def __init__(self, hidden_size=768, conv_ch=64, lstm_hidden=128, lstm_layers=2, out_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, conv_ch, (3, 3), padding=1),\n",
        "            nn.BatchNorm2d(conv_ch),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2))\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(conv_ch, conv_ch * 2, (3, 3), padding=1),\n",
        "            nn.BatchNorm2d(conv_ch * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2))\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(conv_ch * 2, conv_ch * 2, (3, 3), padding=1),\n",
        "            nn.BatchNorm2d(conv_ch * 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=conv_ch * 2,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.attn_fc = nn.Linear(2 * lstm_hidden, 1)\n",
        "        self.out_proj = nn.Linear(2 * lstm_hidden, out_dim)\n",
        "\n",
        "    def forward(self, token_embs):\n",
        "        B, L, H = token_embs.size()\n",
        "        x = token_embs.permute(0, 2, 1).unsqueeze(1)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        x = x.mean(dim=2).permute(0, 2, 1)\n",
        "        outputs, _ = self.lstm(x)\n",
        "\n",
        "        attn_scores = self.attn_fc(outputs).squeeze(-1)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)\n",
        "        pooled = torch.sum(outputs * attn_weights, dim=1)\n",
        "\n",
        "        return self.out_proj(pooled)\n",
        "\n",
        "class TextEmotionModel(nn.Module):\n",
        "    \"\"\"Text emotion recognition model\"\"\"\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.text_proj = nn.Sequential(\n",
        "            nn.Linear(self.bert.config.hidden_size, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.hybrid = TokenCNN_BiLSTM_Attention(out_dim=64)\n",
        "        self.classifier = nn.Linear(128 + 64, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "        text_feat = self.text_proj(bert_out[:, 0, :])\n",
        "        hybrid_feat = self.hybrid(bert_out)\n",
        "        combined = torch.cat([text_feat, hybrid_feat], dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# ============================================================================\n",
        "# AUDIO MODEL ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "class CRNNBranch(nn.Module):\n",
        "    \"\"\"Enhanced CNN-BiLSTM-Attention for audio spectrograms\"\"\"\n",
        "    def __init__(self, in_channels=1, hidden_size=384, out_dim=384):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(0.15)\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 1)),\n",
        "            nn.Dropout2d(0.2)\n",
        "        )\n",
        "\n",
        "        self.feature_size = 256 * 16\n",
        "\n",
        "        self.bilstm = nn.LSTM(\n",
        "            self.feature_size,\n",
        "            hidden_size,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            num_layers=3,\n",
        "            dropout=0.3\n",
        "        )\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            hidden_size * 2,\n",
        "            num_heads=8,\n",
        "            dropout=0.2,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, out_dim),\n",
        "            nn.LayerNorm(out_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        b, c, f, t = x.size()\n",
        "        x = x.permute(0, 3, 1, 2).contiguous().view(b, t, -1)\n",
        "\n",
        "        self.bilstm.flatten_parameters()\n",
        "        x, _ = self.bilstm(x)\n",
        "\n",
        "        x, _ = self.attention(x, x, x)\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        return self.proj(x)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal loss for handling class imbalance\"\"\"\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "class AudioEmotionModel(nn.Module):\n",
        "    \"\"\"Audio emotion recognition model\"\"\"\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "\n",
        "        # Branch A: Wav2Vec2\n",
        "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(DEVICE)\n",
        "        for p in self.wav2vec.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # Branch B: Enhanced CRNN\n",
        "        self.crnn = CRNNBranch(hidden_size=384, out_dim=384)\n",
        "\n",
        "        # Fusion layers\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(768 + 384, 768),\n",
        "            nn.LayerNorm(768),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(768, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.35),\n",
        "            nn.Linear(512, 384),\n",
        "            nn.LayerNorm(384),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(384, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.35),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, wav, mel, wav2vec_emb=None):\n",
        "        # Process wav2vec features\n",
        "        if wav2vec_emb is not None and not any(p.requires_grad for p in self.wav2vec.parameters()):\n",
        "            wav_emb = wav2vec_emb.to(DEVICE)\n",
        "        else:\n",
        "            processor_wav2vec = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\",\n",
        "                                                                   sampling_rate=SAMPLE_RATE)\n",
        "            input_values = processor_wav2vec([w.cpu().numpy() for w in wav],\n",
        "                                            sampling_rate=SAMPLE_RATE,\n",
        "                                            return_tensors=\"pt\",\n",
        "                                            padding=True).input_values.to(DEVICE)\n",
        "\n",
        "            if any(p.requires_grad for p in self.wav2vec.parameters()):\n",
        "                wav_emb = self.wav2vec(input_values).last_hidden_state.mean(dim=1)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    wav_emb = self.wav2vec(input_values).last_hidden_state.mean(dim=1)\n",
        "\n",
        "        # Process spectrogram features\n",
        "        crnn_emb = self.crnn(mel.to(DEVICE))\n",
        "\n",
        "        # Fusion and classification\n",
        "        fused = self.fusion(torch.cat([wav_emb, crnn_emb], dim=1))\n",
        "        return self.classifier(fused)\n",
        "\n",
        "# ============================================================================\n",
        "# TEXT DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "class TextEmotionDataset(Dataset):\n",
        "    \"\"\"Dataset for text emotion recognition\"\"\"\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = [label_encoder.transform([l])[0] for l in labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        toks = tokenizer(text, truncation=True, padding='max_length',\n",
        "                        max_length=MAX_LEN, return_tensors='pt')\n",
        "\n",
        "        input_ids = toks['input_ids'].squeeze(0)\n",
        "        attention_mask = toks['attention_mask'].squeeze(0)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label}\n",
        "\n",
        "def text_collate_fn(batch):\n",
        "    \"\"\"Collate function for text batches\"\"\"\n",
        "    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n",
        "    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch])\n",
        "    labels = torch.stack([b[\"label\"] for b in batch])\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": labels}\n",
        "\n",
        "# ============================================================================\n",
        "# AUDIO DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "processor_audio = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\",\n",
        "                                                     sampling_rate=SAMPLE_RATE)\n",
        "\n",
        "def cache_audio_file_features(wav_path, wav2vec_model=None, processor_obj=None):\n",
        "    \"\"\"Cache audio features to disk\"\"\"\n",
        "    h = hashlib.sha1(wav_path.encode()).hexdigest()[:10]\n",
        "    cache_file = os.path.join(CACHE_DIR, f\"{Path(wav_path).stem}_{h}.npz\")\n",
        "\n",
        "    if os.path.exists(cache_file):\n",
        "        try:\n",
        "            np.load(cache_file)\n",
        "            return cache_file\n",
        "        except:\n",
        "            try:\n",
        "                os.remove(cache_file)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    try:\n",
        "        wav, _ = librosa.load(wav_path, sr=SAMPLE_RATE)\n",
        "        wav = librosa.effects.trim(wav, top_db=25)[0]\n",
        "        wav = librosa.util.normalize(wav)\n",
        "        wav = np.pad(wav, (0, max(0, TARGET_LEN - len(wav))))[:TARGET_LEN]\n",
        "\n",
        "        mel = librosa.feature.melspectrogram(y=wav, sr=SAMPLE_RATE, n_mels=128,\n",
        "                                            n_fft=2048, hop_length=256,\n",
        "                                            fmin=50, fmax=8000)\n",
        "        mel = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "        wav2vec_arr = None\n",
        "        if wav2vec_model and processor_obj:\n",
        "            input_values = processor_obj(wav, sampling_rate=SAMPLE_RATE,\n",
        "                                        return_tensors=\"pt\", padding=True).input_values.to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                out = wav2vec_model(input_values)\n",
        "                wav2vec_arr = out.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy().astype(np.float32)\n",
        "\n",
        "        if wav2vec_arr is None:\n",
        "            np.savez_compressed(cache_file, wav=wav.astype(np.float32), mel=mel.astype(np.float32))\n",
        "        else:\n",
        "            np.savez_compressed(cache_file, wav=wav.astype(np.float32),\n",
        "                              mel=mel.astype(np.float32), wav2vec=wav2vec_arr)\n",
        "\n",
        "        return cache_file\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "class AudioEmotionDataset(Dataset):\n",
        "    \"\"\"Dataset for audio emotion recognition with augmentation\"\"\"\n",
        "    def __init__(self, files, labels, train=True, dataset_name=None):\n",
        "        self.files = files\n",
        "        self.labels = label_encoder.transform(labels)\n",
        "        self.labels_str = labels\n",
        "        self.train = train\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "        # Dataset-specific augmentation multipliers\n",
        "        if dataset_name == \"ShEMO\":\n",
        "            self.aug_multiplier = 2.2\n",
        "        elif dataset_name == \"TESS\":\n",
        "            self.aug_multiplier = 1.8\n",
        "        elif dataset_name == \"RAVDESS\":\n",
        "            self.aug_multiplier = 0.9\n",
        "        elif dataset_name == \"SUBESCO\":\n",
        "            self.aug_multiplier = 1.3\n",
        "        else:\n",
        "            self.aug_multiplier = 1.1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cache = cache_audio_file_features(self.files[idx])\n",
        "\n",
        "        if cache:\n",
        "            data = np.load(cache)\n",
        "            wav = data[\"wav\"]\n",
        "            mel = data[\"mel\"]\n",
        "            wav2vec_arr = data[\"wav2vec\"] if 'wav2vec' in data else None\n",
        "            data.close()\n",
        "        else:\n",
        "            wav = np.zeros(TARGET_LEN, dtype=np.float32)\n",
        "            mel = np.zeros((128, int(TARGET_LEN / 256) + 1), dtype=np.float32)\n",
        "            wav2vec_arr = None\n",
        "\n",
        "        # Apply augmentation during training\n",
        "        if self.train:\n",
        "            # Gaussian noise\n",
        "            if random.random() < NOISE_PROB * self.aug_multiplier:\n",
        "                wav += 0.015 * self.aug_multiplier * np.random.randn(len(wav))\n",
        "\n",
        "            # Volume augmentation\n",
        "            wav *= (0.7 + 0.6 * np.random.random())\n",
        "\n",
        "            # Pitch shifting\n",
        "            if random.random() < PITCH_SHIFT_PROB * self.aug_multiplier:\n",
        "                try:\n",
        "                    n_steps = random.choice([-2, -1, 1, 2])\n",
        "                    wav = librosa.effects.pitch_shift(wav, sr=SAMPLE_RATE, n_steps=n_steps)\n",
        "                    wav = np.pad(wav, (0, max(0, TARGET_LEN - len(wav))))[:TARGET_LEN]\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Time stretching\n",
        "            if random.random() < TIME_STRETCH_PROB * self.aug_multiplier:\n",
        "                try:\n",
        "                    rate = random.uniform(0.85, 1.15)\n",
        "                    wav = librosa.effects.time_stretch(wav, rate=rate)\n",
        "                    wav = np.pad(wav, (0, max(0, TARGET_LEN - len(wav))))[:TARGET_LEN]\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Spectrogram augmentation\n",
        "            mel = self.spec_augment(mel, mask_percent=SPEC_AUG_MASK * self.aug_multiplier)\n",
        "\n",
        "        wav_t = torch.tensor(wav, dtype=torch.float32)\n",
        "        mel_t = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)\n",
        "        wav2_t = torch.tensor(wav2vec_arr, dtype=torch.float32) if wav2vec_arr is not None else None\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        return wav_t, mel_t, wav2_t, label\n",
        "\n",
        "    def spec_augment(self, spec, mask_percent=SPEC_AUG_MASK):\n",
        "        \"\"\"Apply SpecAugment\"\"\"\n",
        "        spec = spec.copy()\n",
        "        T, F = spec.shape[1], spec.shape[0]\n",
        "\n",
        "        # Time masking\n",
        "        if T > 0:\n",
        "            mask_size = int(T * mask_percent)\n",
        "            start = random.randint(0, max(0, T - mask_size))\n",
        "            end = min(T, start + mask_size)\n",
        "            spec[:, start:end] = spec.min()\n",
        "\n",
        "        # Frequency masking\n",
        "        if F > 0:\n",
        "            mask_size = int(F * mask_percent)\n",
        "            start = random.randint(0, max(0, F - mask_size))\n",
        "            end = min(F, start + mask_size)\n",
        "            spec[start:end, :] = spec.min()\n",
        "\n",
        "        return spec\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def train_text_model_one_epoch(model, loader, optimizer, scheduler, criterion):\n",
        "    \"\"\"Train text model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "        labels = batch[\"label\"].to(DEVICE)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=use_amp):\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        if use_amp and scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            optimizer.step()\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "def train_audio_model_one_epoch(model, loader, optimizer, criterion,\n",
        "                                global_state_for_prox=None, mu=0.0, use_mixup=True):\n",
        "    \"\"\"Train audio model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    preds, trues = [], []\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        wav, mel, wav2vec_emb, y = batch\n",
        "        wav, mel, y = wav.to(DEVICE), mel.to(DEVICE), y.to(DEVICE)\n",
        "        if wav2vec_emb is not None:\n",
        "            wav2vec_emb = wav2vec_emb.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Apply mixup\n",
        "        if use_mixup and random.random() < MIXUP_PROB:\n",
        "            wav, mel, wav2vec_emb, y1, y2, lam = mixup_batch_audio((wav, mel, wav2vec_emb, y))\n",
        "            y1, y2 = y1.to(DEVICE), y2.to(DEVICE)\n",
        "            use_mixup_this_batch = True\n",
        "        else:\n",
        "            y1 = y2 = None\n",
        "            lam = None\n",
        "            use_mixup_this_batch = False\n",
        "\n",
        "        if use_amp and scaler:\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                logits = model(wav, mel, wav2vec_emb)\n",
        "                if lam is None:\n",
        "                    loss = criterion(logits, y)\n",
        "                else:\n",
        "                    loss = lam * criterion(logits, y1) + (1 - lam) * criterion(logits, y2)\n",
        "\n",
        "                # FedProx regularization\n",
        "                if mu > 0 and global_state_for_prox is not None:\n",
        "                    prox = 0.0\n",
        "                    for (k, p) in model.state_dict().items():\n",
        "                        prox += torch.sum((p.to(DEVICE) - global_state_for_prox[k].to(DEVICE)) ** 2)\n",
        "                    loss = loss + (mu / 2.0) * prox\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model(wav, mel, wav2vec_emb)\n",
        "            if lam is None:\n",
        "                loss = criterion(logits, y)\n",
        "            else:\n",
        "                loss = lam * criterion(logits, y1) + (1 - lam) * criterion(logits, y2)\n",
        "\n",
        "            if mu > 0 and global_state_for_prox is not None:\n",
        "                prox = 0.0\n",
        "                for (k, p) in model.state_dict().items():\n",
        "                    prox += torch.sum((p.to(DEVICE) - global_state_for_prox[k].to(DEVICE)) ** 2)\n",
        "                loss = loss + (mu / 2.0) * prox\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            optimizer.step()\n",
        "\n",
        "        batch_n = wav.size(0)\n",
        "        total_loss += loss.item() * batch_n\n",
        "        total_samples += batch_n\n",
        "\n",
        "        preds.extend(logits.argmax(1).cpu().numpy())\n",
        "        trues.extend(y.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_samples)\n",
        "    acc, f1, recall, precision = compute_metrics(trues, preds)\n",
        "    return avg_loss, acc, f1, recall, precision, trues, preds\n",
        "\n",
        "def evaluate_text_model(model, loader, criterion):\n",
        "    \"\"\"Evaluate text model\"\"\"\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    preds_list, labels_list = [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "            labels = batch[\"label\"].to(DEVICE)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            preds_list.extend(preds.cpu().numpy())\n",
        "            labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = total_correct / total_samples\n",
        "    loss = total_loss / total_samples\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels_list, preds_list, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    return acc, loss, precision, recall, f1, labels_list, preds_list\n",
        "\n",
        "def evaluate_audio_model(model, loader, criterion, device):\n",
        "    \"\"\"Evaluate audio model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    preds, trues = [], []\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "        for wav, mel, wav2vec_emb, y in loader:\n",
        "            wav, mel, y = wav.to(device), mel.to(device), y.to(device)\n",
        "            if wav2vec_emb is not None:\n",
        "                wav2vec_emb = wav2vec_emb.to(device)\n",
        "\n",
        "            logits = model(wav, mel, wav2vec_emb)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "            batch_n = wav.size(0)\n",
        "            total_loss += loss.item() * batch_n\n",
        "            total_samples += batch_n\n",
        "            preds.extend(logits.argmax(1).cpu().numpy())\n",
        "            trues.extend(y.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_samples)\n",
        "    acc, f1, recall, precision = compute_metrics(trues, preds)\n",
        "    return avg_loss, acc, f1, recall, precision, trues, preds\n",
        "\n",
        "# ============================================================================\n",
        "# PREPARE DATALOADERS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREPARING DATALOADERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Text dataloaders\n",
        "text_client_loaders = {}\n",
        "text_val_loaders = {}\n",
        "\n",
        "for name in text_client_texts.keys():\n",
        "    ds = TextEmotionDataset(text_client_texts[name], text_client_labels[name])\n",
        "    val_size = int(0.2 * len(ds))\n",
        "    train_size = len(ds) - val_size\n",
        "    train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
        "\n",
        "    text_client_loaders[name] = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, collate_fn=text_collate_fn)\n",
        "    text_val_loaders[name] = DataLoader(val_ds, batch_size=BATCH_SIZE * 2,\n",
        "                                       shuffle=False, collate_fn=text_collate_fn)\n",
        "    print(f\"  ✓ Text - {name}: Train={len(train_ds)}, Val={len(val_ds)}\")\n",
        "\n",
        "# Audio dataloaders\n",
        "print(\"\\nCaching audio features...\")\n",
        "if audio_clients_files:\n",
        "    wav2vec_cache = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(DEVICE)\n",
        "    wav2vec_cache.eval()\n",
        "    for p in wav2vec_cache.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    all_audio_files = [f for files in audio_clients_files.values() for f in files]\n",
        "\n",
        "    def cache_file_wrapper(fpath):\n",
        "        return cache_audio_file_features(fpath, wav2vec_cache, processor_audio)\n",
        "\n",
        "    max_workers = min(8, os.cpu_count() or 1)\n",
        "    with tqdm(total=len(all_audio_files), desc=\"Caching\", unit=\"file\") as pbar:\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = {executor.submit(cache_file_wrapper, fpath): fpath\n",
        "                      for fpath in all_audio_files}\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    future.result()\n",
        "                except:\n",
        "                    pass\n",
        "                pbar.update(1)\n",
        "\n",
        "    cleanup_model(wav2vec_cache)\n",
        "    aggressive_cleanup()\n",
        "\n",
        "audio_client_loaders = {}\n",
        "audio_val_loaders = {}\n",
        "\n",
        "for name in audio_clients_files.keys():\n",
        "    ds = AudioEmotionDataset(audio_clients_files[name], audio_clients_labels[name],\n",
        "                            train=True, dataset_name=name)\n",
        "    val_size = max(15, int(len(ds) * 0.18))\n",
        "    train_size = len(ds) - val_size\n",
        "    train_ds, val_ds = random_split(ds, [train_size, val_size])\n",
        "    train_ds.dataset.train = True\n",
        "    val_ds.dataset.train = False\n",
        "\n",
        "    audio_client_loaders[name] = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True, num_workers=NUM_WORKERS,\n",
        "                                           pin_memory=True)\n",
        "    audio_val_loaders[name] = DataLoader(val_ds, batch_size=BATCH_SIZE,\n",
        "                                        shuffle=False, num_workers=NUM_WORKERS,\n",
        "                                        pin_memory=True)\n",
        "    print(f\"  ✓ Audio - {name}: Train={len(train_ds)}, Val={len(val_ds)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FEDERATED LEARNING MAIN LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING UNIFIED FEDERATED LEARNING\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Text clients: {list(text_client_texts.keys())}\")\n",
        "print(f\"Audio clients: {list(audio_clients_files.keys())}\")\n",
        "print(f\"Rounds: {ROUNDS}, Local Epochs: {LOCAL_EPOCHS}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "\n",
        "# Initialize global models\n",
        "text_global_model = TextEmotionModel().to(DEVICE) if text_client_texts else None\n",
        "audio_global_model = AudioEmotionModel().to(DEVICE) if audio_clients_files else None\n",
        "\n",
        "# Loss functions\n",
        "text_criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "audio_criterion = FocalLoss(alpha=class_weights_tensor, gamma=2.0)\n",
        "\n",
        "# Track best models\n",
        "best_text_acc = 0.0\n",
        "best_audio_acc = 0.0\n",
        "best_text_state = None\n",
        "best_audio_state = None\n",
        "\n",
        "# History tracking for plots\n",
        "text_history = {\n",
        "    'global_acc': [],\n",
        "    'global_loss': [],\n",
        "    'global_f1': [],\n",
        "    'client_acc': {name: [] for name in text_client_texts.keys()},\n",
        "    'client_loss': {name: [] for name in text_client_texts.keys()},\n",
        "    'client_f1': {name: [] for name in text_client_texts.keys()}\n",
        "}\n",
        "\n",
        "audio_history = {\n",
        "    'global_acc': [],\n",
        "    'global_loss': [],\n",
        "    'global_f1': [],\n",
        "    'client_acc': {name: [] for name in audio_clients_files.keys()},\n",
        "    'client_loss': {name: [] for name in audio_clients_files.keys()},\n",
        "    'client_f1': {name: [] for name in audio_clients_files.keys()}\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "for rnd in range(ROUNDS):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ROUND {rnd + 1}/{ROUNDS}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    lr_mult = get_lr_multiplier(rnd, WARMUP_ROUNDS, ROUNDS)\n",
        "    print(f\"LR multiplier: {lr_mult:.4f}\")\n",
        "\n",
        "    # ========================= TEXT TRAINING =========================\n",
        "    if text_global_model and text_client_loaders:\n",
        "        print(f\"\\n--- TEXT MODALITY ---\")\n",
        "        text_local_states = []\n",
        "        text_client_sizes = []\n",
        "\n",
        "        for cname in text_client_texts.keys():\n",
        "            print(f\"\\nTraining text client: {cname}\")\n",
        "            local_model = TextEmotionModel().to(DEVICE)\n",
        "            local_model.load_state_dict(text_global_model.state_dict())\n",
        "\n",
        "            optimizer = AdamW(\n",
        "                local_model.parameters(),\n",
        "                lr=LR_TEXT_BERT * lr_mult,\n",
        "                weight_decay=WEIGHT_DECAY\n",
        "            )\n",
        "\n",
        "            scheduler = get_linear_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=int(WARMUP_RATIO * len(text_client_loaders[cname]) * LOCAL_EPOCHS),\n",
        "                num_training_steps=len(text_client_loaders[cname]) * LOCAL_EPOCHS\n",
        "            )\n",
        "\n",
        "            for ep in range(LOCAL_EPOCHS):\n",
        "                loss, acc = train_text_model_one_epoch(\n",
        "                    local_model, text_client_loaders[cname],\n",
        "                    optimizer, scheduler, text_criterion\n",
        "                )\n",
        "                print(f\"  Epoch {ep+1}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
        "\n",
        "            text_local_states.append(deepcopy(local_model.state_dict()))\n",
        "            text_client_sizes.append(len(text_client_loaders[cname].dataset))\n",
        "            cleanup_model(local_model)\n",
        "\n",
        "        # FedAvg for text\n",
        "        print(\"\\nAggregating text models...\")\n",
        "        text_total = sum(text_client_sizes)\n",
        "        new_text_state = deepcopy(text_global_model.state_dict())\n",
        "\n",
        "        for key in new_text_state:\n",
        "            new_text_state[key] = sum(\n",
        "                text_local_states[i][key] * (text_client_sizes[i] / text_total)\n",
        "                for i in range(len(text_client_sizes))\n",
        "            )\n",
        "\n",
        "        text_global_model.load_state_dict(new_text_state)\n",
        "\n",
        "        # Evaluate text model\n",
        "        print(\"\\nText validation results:\")\n",
        "        text_round_accs = []\n",
        "        text_round_losses = []\n",
        "        text_round_f1s = []\n",
        "\n",
        "        for cname in text_client_texts.keys():\n",
        "            acc, loss, prec, rec, f1, _, _ = evaluate_text_model(\n",
        "                text_global_model, text_val_loaders[cname], text_criterion\n",
        "            )\n",
        "            print(f\"  {cname}: Acc={acc:.4f}, F1={f1:.4f}, Loss={loss:.4f}\")\n",
        "\n",
        "            # Track client history\n",
        "            text_history['client_acc'][cname].append(acc)\n",
        "            text_history['client_loss'][cname].append(loss)\n",
        "            text_history['client_f1'][cname].append(f1)\n",
        "\n",
        "            text_round_accs.append(acc)\n",
        "            text_round_losses.append(loss)\n",
        "            text_round_f1s.append(f1)\n",
        "\n",
        "            if acc > best_text_acc:\n",
        "                best_text_acc = acc\n",
        "                best_text_state = deepcopy(text_global_model.state_dict())\n",
        "\n",
        "        # Track global (average) history\n",
        "        text_history['global_acc'].append(np.mean(text_round_accs))\n",
        "        text_history['global_loss'].append(np.mean(text_round_losses))\n",
        "        text_history['global_f1'].append(np.mean(text_round_f1s))\n",
        "\n",
        "        print(f\"\\n  Global Text Avg: Acc={np.mean(text_round_accs):.4f}, \"\n",
        "              f\"F1={np.mean(text_round_f1s):.4f}, Loss={np.mean(text_round_losses):.4f}\")\n",
        "\n",
        "    # ========================= AUDIO TRAINING =========================\n",
        "    if audio_global_model and audio_client_loaders:\n",
        "        print(f\"\\n--- AUDIO MODALITY ---\")\n",
        "        audio_local_states = []\n",
        "        audio_client_sizes = []\n",
        "        audio_global_state_for_prox = {k: v.clone().detach().to('cpu')\n",
        "                                      for k, v in audio_global_model.state_dict().items()}\n",
        "\n",
        "        for cname in audio_clients_files.keys():\n",
        "            print(f\"\\nTraining audio client: {cname}\")\n",
        "            local_model = deepcopy(audio_global_model).to(DEVICE)\n",
        "\n",
        "            if any(p.requires_grad for p in local_model.wav2vec.parameters()):\n",
        "                optimizer = torch.optim.AdamW([\n",
        "                    {\"params\": local_model.wav2vec.parameters(), \"lr\": LR_AUDIO_WAV2VEC * lr_mult},\n",
        "                    {\"params\": list(local_model.crnn.parameters()) +\n",
        "                              list(local_model.fusion.parameters()) +\n",
        "                              list(local_model.classifier.parameters()),\n",
        "                     \"lr\": LR_AUDIO_NEW * lr_mult}\n",
        "                ], weight_decay=WEIGHT_DECAY)\n",
        "            else:\n",
        "                optimizer = torch.optim.AdamW(\n",
        "                    list(local_model.crnn.parameters()) +\n",
        "                    list(local_model.fusion.parameters()) +\n",
        "                    list(local_model.classifier.parameters()),\n",
        "                    lr=LR_AUDIO_NEW * lr_mult,\n",
        "                    weight_decay=WEIGHT_DECAY\n",
        "                )\n",
        "\n",
        "            for ep in range(LOCAL_EPOCHS):\n",
        "                loss, acc, f1, rec, prec, _, _ = train_audio_model_one_epoch(\n",
        "                    local_model, audio_client_loaders[cname], optimizer,\n",
        "                    audio_criterion, audio_global_state_for_prox, FEDPROX_MU\n",
        "                )\n",
        "                print(f\"  Epoch {ep+1}: Loss={loss:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "            audio_local_states.append({k: v.cpu() for k, v in local_model.state_dict().items()})\n",
        "            audio_client_sizes.append(len(audio_client_loaders[cname].dataset))\n",
        "            cleanup_model(local_model)\n",
        "\n",
        "        # FedAvg for audio\n",
        "        print(\"\\nAggregating audio models...\")\n",
        "        audio_total = sum(audio_client_sizes)\n",
        "        new_audio_state = deepcopy(audio_global_model.state_dict())\n",
        "\n",
        "        for key in new_audio_state:\n",
        "            acc = None\n",
        "            for idx in range(len(audio_client_sizes)):\n",
        "                part = audio_local_states[idx][key].float() * (audio_client_sizes[idx] / audio_total)\n",
        "                acc = part if acc is None else (acc + part)\n",
        "            new_audio_state[key] = acc\n",
        "\n",
        "        audio_global_model.load_state_dict(new_audio_state)\n",
        "\n",
        "        # Evaluate audio model\n",
        "        print(\"\\nAudio validation results:\")\n",
        "        audio_round_accs = []\n",
        "        audio_round_losses = []\n",
        "        audio_round_f1s = []\n",
        "\n",
        "        for cname in audio_clients_files.keys():\n",
        "            loss, acc, f1, rec, prec, _, _ = evaluate_audio_model(\n",
        "                audio_global_model, audio_val_loaders[cname], audio_criterion, DEVICE\n",
        "            )\n",
        "            print(f\"  {cname}: Acc={acc:.4f}, F1={f1:.4f}, Loss={loss:.4f}\")\n",
        "\n",
        "            # Track client history\n",
        "            audio_history['client_acc'][cname].append(acc)\n",
        "            audio_history['client_loss'][cname].append(loss)\n",
        "            audio_history['client_f1'][cname].append(f1)\n",
        "\n",
        "            audio_round_accs.append(acc)\n",
        "            audio_round_losses.append(loss)\n",
        "            audio_round_f1s.append(f1)\n",
        "\n",
        "            if acc > best_audio_acc:\n",
        "                best_audio_acc = acc\n",
        "                best_audio_state = deepcopy(audio_global_model.state_dict())\n",
        "\n",
        "        # Track global (average) history\n",
        "        audio_history['global_acc'].append(np.mean(audio_round_accs))\n",
        "        audio_history['global_loss'].append(np.mean(audio_round_losses))\n",
        "        audio_history['global_f1'].append(np.mean(audio_round_f1s))\n",
        "\n",
        "        print(f\"\\n  Global Audio Avg: Acc={np.mean(audio_round_accs):.4f}, \"\n",
        "              f\"F1={np.mean(audio_round_f1s):.4f}, Loss={np.mean(audio_round_losses):.4f}\")\n",
        "\n",
        "    aggressive_cleanup()\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "SAVE_DIR = \"/content/saved_models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save Text Model (if it exists)\n",
        "if text_global_model and best_text_state:\n",
        "    text_global_model.load_state_dict(best_text_state)\n",
        "\n",
        "    torch.save({\n",
        "        'model_state_dict': text_global_model.state_dict(),\n",
        "        'best_accuracy': best_text_acc,\n",
        "        'label_encoder': label_encoder,\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotion_classes': list(label_encoder.classes_),\n",
        "        'config': {\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'max_len': MAX_LEN,\n",
        "            'rounds': ROUNDS,\n",
        "            'num_classes': NUM_CLASSES\n",
        "        }\n",
        "    }, os.path.join(SAVE_DIR, 'text_emotion_model.pth'))\n",
        "\n",
        "    print(f\"✓ Text Model Saved!\")\n",
        "    print(f\"  Accuracy: {best_text_acc:.4f}\")\n",
        "    print(f\"  Path: {SAVE_DIR}/text_emotion_model.pth\")\n",
        "\n",
        "# Save Audio Model (if it exists)\n",
        "if audio_global_model and best_audio_state:\n",
        "    audio_global_model.load_state_dict(best_audio_state)\n",
        "\n",
        "    torch.save({\n",
        "        'model_state_dict': audio_global_model.state_dict(),\n",
        "        'best_accuracy': best_audio_acc,\n",
        "        'label_encoder': label_encoder,\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotion_classes': list(label_encoder.classes_),\n",
        "        'config': {\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'sample_rate': SAMPLE_RATE,\n",
        "            'target_len': TARGET_LEN,\n",
        "            'rounds': ROUNDS,\n",
        "            'num_classes': NUM_CLASSES\n",
        "        }\n",
        "    }, os.path.join(SAVE_DIR, 'audio_emotion_model.pth'))\n",
        "\n",
        "    print(f\"✓ Audio Model Saved!\")\n",
        "    print(f\"  Accuracy: {best_audio_acc:.4f}\")\n",
        "    print(f\"  Path: {SAVE_DIR}/audio_emotion_model.pth\")\n",
        "\n",
        "print(f\"\\n✓ Models saved to: {SAVE_DIR}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"UNIFIED FEDERATED LEARNING SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Modalities: {'Text' if text_global_model else ''} \"\n",
        "      f\"{'Audio' if audio_global_model else ''}\")\n",
        "print(f\"Total rounds: {ROUNDS}\")\n",
        "print(f\"Unified emotion classes: {list(label_encoder.classes_)}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# ============================================================================\n",
        "# PLOT ACCURACY GRAPHS\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_history(history, modality_name, save_dir=CACHE_DIR):\n",
        "    \"\"\"Plot training history for server (global) and clients\"\"\"\n",
        "\n",
        "    if not history['global_acc']:\n",
        "        print(f\"No history to plot for {modality_name}\")\n",
        "        return\n",
        "\n",
        "    rounds = list(range(1, len(history['global_acc']) + 1))\n",
        "\n",
        "    # Set style\n",
        "    try:\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    except:\n",
        "        try:\n",
        "            plt.style.use('seaborn-darkgrid')\n",
        "        except:\n",
        "            pass  # Use default style\n",
        "    sns.set_palette(\"husl\")\n",
        "\n",
        "    # Create figure with 3 subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "    fig.suptitle(f'{modality_name} Emotion Recognition - Federated Learning',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Accuracy plot\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(rounds, history['global_acc'], 'b-', linewidth=3,\n",
        "             marker='o', markersize=8, label='Global Server', alpha=0.8)\n",
        "\n",
        "    for client_name, client_accs in history['client_acc'].items():\n",
        "        ax1.plot(rounds, client_accs, '--', linewidth=2,\n",
        "                marker='s', markersize=5, label=f'Client: {client_name}', alpha=0.7)\n",
        "\n",
        "    ax1.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Accuracy Progression', fontsize=13, fontweight='bold')\n",
        "    ax1.legend(loc='best', fontsize=9)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim([0, 1])\n",
        "\n",
        "    # 2. Loss plot\n",
        "    ax2 = axes[1]\n",
        "    ax2.plot(rounds, history['global_loss'], 'r-', linewidth=3,\n",
        "             marker='o', markersize=8, label='Global Server', alpha=0.8)\n",
        "\n",
        "    for client_name, client_losses in history['client_loss'].items():\n",
        "        ax2.plot(rounds, client_losses, '--', linewidth=2,\n",
        "                marker='s', markersize=5, label=f'Client: {client_name}', alpha=0.7)\n",
        "\n",
        "    ax2.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Loss Progression', fontsize=13, fontweight='bold')\n",
        "    ax2.legend(loc='best', fontsize=9)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. F1-Score plot\n",
        "    ax3 = axes[2]\n",
        "    ax3.plot(rounds, history['global_f1'], 'g-', linewidth=3,\n",
        "             marker='o', markersize=8, label='Global Server', alpha=0.8)\n",
        "\n",
        "    for client_name, client_f1s in history['client_f1'].items():\n",
        "        ax3.plot(rounds, client_f1s, '--', linewidth=2,\n",
        "                marker='s', markersize=5, label=f'Client: {client_name}', alpha=0.7)\n",
        "\n",
        "    ax3.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
        "    ax3.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
        "    ax3.set_title('F1-Score Progression', fontsize=13, fontweight='bold')\n",
        "    ax3.legend(loc='best', fontsize=9)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.set_ylim([0, 1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    save_path = os.path.join(save_dir, f'{modality_name.lower()}_training_history.png')\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n✓ {modality_name} training plot saved to: {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Create detailed comparison plot\n",
        "    fig2, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    # Plot global server line prominently\n",
        "    ax.plot(rounds, history['global_acc'], 'b-', linewidth=4,\n",
        "            marker='o', markersize=10, label='Global Server (FedAvg)',\n",
        "            alpha=0.9, zorder=10)\n",
        "\n",
        "    # Plot each client\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(history['client_acc'])))\n",
        "    for idx, (client_name, client_accs) in enumerate(history['client_acc'].items()):\n",
        "        ax.plot(rounds, client_accs, '--', linewidth=2.5,\n",
        "                marker='D', markersize=6, label=f'{client_name}',\n",
        "                alpha=0.7, color=colors[idx])\n",
        "\n",
        "    ax.set_xlabel('Federated Round', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    ax.set_title(f'{modality_name} - Server vs Client Accuracy Comparison',\n",
        "                 fontsize=15, fontweight='bold')\n",
        "    ax.legend(loc='lower right', fontsize=11, framealpha=0.9)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.axhline(y=0.8, color='red', linestyle=':', linewidth=2,\n",
        "               label='80% Target', alpha=0.5)\n",
        "\n",
        "    # Add annotations for best performance\n",
        "    best_global_idx = np.argmax(history['global_acc'])\n",
        "    best_global_acc = history['global_acc'][best_global_idx]\n",
        "    ax.annotate(f'Best: {best_global_acc:.3f}',\n",
        "                xy=(best_global_idx + 1, best_global_acc),\n",
        "                xytext=(10, 10), textcoords='offset points',\n",
        "                bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n",
        "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'),\n",
        "                fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save comparison figure\n",
        "    save_path2 = os.path.join(save_dir, f'{modality_name.lower()}_accuracy_comparison.png')\n",
        "    plt.savefig(save_path2, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✓ {modality_name} comparison plot saved to: {save_path2}\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING TRAINING VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Plot text results\n",
        "if text_global_model and text_history['global_acc']:\n",
        "    plot_training_history(text_history, 'Text')\n",
        "\n",
        "# Plot audio results\n",
        "if audio_global_model and audio_history['global_acc']:\n",
        "    plot_training_history(audio_history, 'Audio')\n",
        "\n",
        "# Create combined comparison if both modalities exist\n",
        "if (text_global_model and text_history['global_acc'] and\n",
        "    audio_global_model and audio_history['global_acc']):\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    fig.suptitle('Multimodal Federated Learning - Text vs Audio',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "\n",
        "    rounds_text = list(range(1, len(text_history['global_acc']) + 1))\n",
        "    rounds_audio = list(range(1, len(audio_history['global_acc']) + 1))\n",
        "\n",
        "    # Text modality\n",
        "    ax1.plot(rounds_text, text_history['global_acc'], 'b-', linewidth=3,\n",
        "             marker='o', markersize=8, label='Server', alpha=0.8)\n",
        "    for client_name, client_accs in text_history['client_acc'].items():\n",
        "        ax1.plot(rounds_text, client_accs, '--', linewidth=2,\n",
        "                marker='s', markersize=5, label=client_name, alpha=0.7)\n",
        "    ax1.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Text Emotion Recognition', fontsize=13, fontweight='bold')\n",
        "    ax1.legend(loc='best', fontsize=9)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim([0, 1])\n",
        "    ax1.axhline(y=0.8, color='red', linestyle=':', linewidth=2, alpha=0.5)\n",
        "\n",
        "    # Audio modality\n",
        "    ax2.plot(rounds_audio, audio_history['global_acc'], 'b-', linewidth=3,\n",
        "             marker='o', markersize=8, label='Server', alpha=0.8)\n",
        "    for client_name, client_accs in audio_history['client_acc'].items():\n",
        "        ax2.plot(rounds_audio, client_accs, '--', linewidth=2,\n",
        "                marker='s', markersize=5, label=client_name, alpha=0.7)\n",
        "    ax2.set_xlabel('Round', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Audio Emotion Recognition', fontsize=13, fontweight='bold')\n",
        "    ax2.legend(loc='best', fontsize=9)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim([0, 1])\n",
        "    ax2.axhline(y=0.8, color='red', linestyle=':', linewidth=2, alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(CACHE_DIR, 'multimodal_comparison.png')\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n✓ Multimodal comparison plot saved to: {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL VISUALIZATIONS GENERATED SUCCESSFULLY\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "06cc2acb47f84537af1af28ff300314e",
            "acd1e428f54e4e5a90325553ba72289e",
            "8d150ac54729467c8d49187593973640",
            "fbd43f87db09460d8cebcc0858572719",
            "16bd0982e824453aaf9245de24214360",
            "f955de3c61574e80b03514d03ced24df",
            "96f84fa042254ddda3e4455ed8a22b2b",
            "d007d45dc30d44ce8b027524de5115fc",
            "54f741d062c7430f8b06b39b2fa11b42",
            "b3864ed176e248038936bdb552f10793",
            "198ac2c7e1de47c6a51e5618da3a6ebb",
            "30bb2014a5c2475ba0ad804949ab55e2",
            "b4543cf139dd42d7ae32c5380ec8ffcf",
            "e6f35bd4c2c844f58c5677074303e2a2",
            "72448f4ec9e44a9ca620ac93c3e5c722",
            "7d58aeee224049abb6cd34a0171c8cf5",
            "b322ec1216eb43cb8ef2df9712caf35d",
            "e8b9a1772cf84f8d962a356dfa32fb15",
            "b2496bdb6f7f4612ba69cc6680ab747e",
            "a5715d7719c14975b2a6d5eb4211a7ef",
            "86d88460d2944f28be64315644ff3bac",
            "0007f2d7befa4c5197c3aea75f3dab6f"
          ]
        },
        "id": "0JXRBadydc74",
        "outputId": "9f5cb854-9619-4c1a-a65d-8ac49e42d5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing required packages...\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Package installation complete!\n",
            "================================================================================\n",
            "\n",
            "Device: cuda\n",
            "Batch Size: 32, Mixed Precision: True\n",
            "\n",
            "================================================================================\n",
            "LOADING TEXT DATASETS\n",
            "================================================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:152: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if use_amp else None\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Colab cache for faster access to the 'emotion-detection-from-text' dataset.\n",
            "✓ Loaded Kaggle text dataset: 40000 samples\n",
            "  ✓ dair-ai/emotion: 15996 samples\n",
            "  ✓ boltuix/emotions-dataset: 20000 samples\n",
            "  ✓ mteb/emotion: 15956 samples\n",
            "  ✓ go_emotions: 13542 samples\n",
            "  ✓ pashupatigupta/emotion: 20000 samples\n",
            "Loaded 5 text datasets\n",
            "\n",
            "================================================================================\n",
            "LOADING AUDIO DATASETS\n",
            "================================================================================\n",
            "  ✓ CREMA-D: 7442 files | Dist: {'disgust': 1271, 'happy': 1271, 'sad': 1271, 'neutral': 1087, 'fear': 1271, 'anger': 1271}\n",
            "  ✓ RAVDESS: 2880 files | Dist: {'surprise': 384, 'neutral': 576, 'disgust': 384, 'fear': 384, 'sad': 384, 'happy': 384, 'anger': 384}\n",
            "  ✓ TESS: 2800 files | Dist: {'fear': 400, 'anger': 400, 'disgust': 400, 'neutral': 400, 'sad': 400, 'surprise': 400, 'happy': 400}\n",
            "  ✓ ShEMO-Male: 1737 files | Dist: {'neutral': 744, 'anger': 604, 'sad': 178, 'fear': 16, 'surprise': 105, 'happy': 90}\n",
            "  ✓ ShEMO-Female: 1263 files | Dist: {'anger': 455, 'neutral': 284, 'sad': 271, 'surprise': 120, 'fear': 22, 'happy': 111}\n",
            "  ✓ SUBESCO: 6000 files | Dist: {'fear': 1000, 'surprise': 1000, 'anger': 1000, 'neutral': 1000, 'sad': 1000, 'happy': 1000}\n",
            "  ✓ ShEMO (Combined): 3000 files | Dist: {'neutral': 1028, 'anger': 1059, 'sad': 449, 'fear': 38, 'surprise': 225, 'happy': 201}\n",
            "Loaded 5 audio datasets\n",
            "\n",
            "================================================================================\n",
            "ENCODING LABELS\n",
            "================================================================================\n",
            "Unified emotion classes (7): [np.str_('anger'), np.str_('disgust'), np.str_('fear'), np.str_('happy'), np.str_('neutral'), np.str_('sad'), np.str_('surprise')]\n",
            "Class weights: {np.str_('anger'): np.float64(1.123), np.str_('disgust'): np.float64(4.591), np.str_('fear'): np.float64(0.901), np.str_('happy'): np.float64(0.513), np.str_('neutral'): np.float64(1.296), np.str_('sad'): np.float64(0.596), np.str_('surprise'): np.float64(2.628)}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "PREPARING DATALOADERS\n",
            "================================================================================\n",
            "  ✓ Text - dair-ai/emotion: Train=12797, Val=3199\n",
            "  ✓ Text - boltuix/emotions-dataset: Train=16000, Val=4000\n",
            "  ✓ Text - mteb/emotion: Train=12765, Val=3191\n",
            "  ✓ Text - go_emotions: Train=10834, Val=2708\n",
            "  ✓ Text - pashupatigupta/emotion: Train=16000, Val=4000\n",
            "\n",
            "Caching audio features...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06cc2acb47f84537af1af28ff300314e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rCaching:   0%|          | 0/22122 [00:00<?, ?file/s]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30bb2014a5c2475ba0ad804949ab55e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Caching: 100%|██████████| 22122/22122 [16:50<00:00, 21.89file/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ Audio - CREMA-D: Train=6103, Val=1339\n",
            "  ✓ Audio - RAVDESS: Train=2362, Val=518\n",
            "  ✓ Audio - TESS: Train=2296, Val=504\n",
            "  ✓ Audio - SUBESCO: Train=4920, Val=1080\n",
            "  ✓ Audio - ShEMO: Train=2460, Val=540\n",
            "\n",
            "================================================================================\n",
            "STARTING UNIFIED FEDERATED LEARNING\n",
            "================================================================================\n",
            "Text clients: ['dair-ai/emotion', 'boltuix/emotions-dataset', 'mteb/emotion', 'go_emotions', 'pashupatigupta/emotion']\n",
            "Audio clients: ['CREMA-D', 'RAVDESS', 'TESS', 'SUBESCO', 'ShEMO']\n",
            "Rounds: 30, Local Epochs: 3\n",
            "Batch Size: 32\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ROUND 1/30\n",
            "================================================================================\n",
            "LR multiplier: 0.5000\n",
            "\n",
            "--- TEXT MODALITY ---\n",
            "\n",
            "Training text client: dair-ai/emotion\n",
            "  Epoch 1: Loss=1.2440, Acc=0.6344\n",
            "  Epoch 2: Loss=0.3799, Acc=0.9203\n",
            "  Epoch 3: Loss=0.2345, Acc=0.9492\n",
            "\n",
            "Training text client: boltuix/emotions-dataset\n",
            "  Epoch 1: Loss=1.5388, Acc=0.3805\n",
            "  Epoch 2: Loss=1.1403, Acc=0.5897\n",
            "  Epoch 3: Loss=1.0013, Acc=0.6486\n",
            "\n",
            "Training text client: mteb/emotion\n",
            "  Epoch 1: Loss=1.2325, Acc=0.6410\n",
            "  Epoch 2: Loss=0.3664, Acc=0.9246\n",
            "  Epoch 3: Loss=0.2266, Acc=0.9528\n",
            "\n",
            "Training text client: go_emotions\n",
            "  Epoch 1: Loss=1.4009, Acc=0.4497\n",
            "  Epoch 2: Loss=0.8550, Acc=0.6963\n",
            "  Epoch 3: Loss=0.7215, Acc=0.7464\n",
            "\n",
            "Training text client: pashupatigupta/emotion\n",
            "  Epoch 1: Loss=1.6678, Acc=0.3064\n",
            "  Epoch 2: Loss=1.4850, Acc=0.3747\n",
            "  Epoch 3: Loss=1.4203, Acc=0.4053\n",
            "\n",
            "Aggregating text models...\n",
            "\n",
            "Text validation results:\n",
            "  dair-ai/emotion: Acc=0.7931, F1=0.6745, Loss=0.9156\n",
            "  boltuix/emotions-dataset: Acc=0.5270, F1=0.3439, Loss=2.1077\n",
            "  mteb/emotion: Acc=0.7991, F1=0.7061, Loss=0.9187\n",
            "  go_emotions: Acc=0.2190, F1=0.1623, Loss=1.9835\n",
            "  pashupatigupta/emotion: Acc=0.2823, F1=0.1953, Loss=1.9893\n",
            "\n",
            "  Global Text Avg: Acc=0.5241, F1=0.4164, Loss=1.5830\n",
            "\n",
            "--- AUDIO MODALITY ---\n",
            "\n",
            "Training audio client: CREMA-D\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 1: Loss=1102.7804, Acc=0.1711, F1=0.0563\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 2: Loss=1537.0781, Acc=0.1847, F1=0.0795\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 3: Loss=2166.4069, Acc=0.2027, F1=0.1045\n",
            "\n",
            "Training audio client: RAVDESS\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 1: Loss=1034.2343, Acc=0.1406, F1=0.0780\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 2: Loss=1295.0557, Acc=0.1363, F1=0.0487\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 3: Loss=1420.1609, Acc=0.1389, F1=0.0534\n",
            "\n",
            "Training audio client: TESS\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 1: Loss=728.6101, Acc=0.1363, F1=0.0619\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 2: Loss=933.8780, Acc=0.1524, F1=0.0578\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 3: Loss=1048.0684, Acc=0.1468, F1=0.0520\n",
            "\n",
            "Training audio client: SUBESCO\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 1: Loss=1259.8497, Acc=0.1665, F1=0.0802\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 2: Loss=1585.1594, Acc=0.2083, F1=0.1155\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 3: Loss=2051.6057, Acc=0.2606, F1=0.1387\n",
            "\n",
            "Training audio client: ShEMO\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 1: Loss=913.5021, Acc=0.3407, F1=0.2819\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 2: Loss=1217.8276, Acc=0.3524, F1=0.2928\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch 3: Loss=1293.3707, Acc=0.3476, F1=0.2915\n",
            "\n",
            "Aggregating audio models...\n",
            "\n",
            "Audio validation results:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  CREMA-D: Acc=0.2651, F1=0.1326, Loss=2.1553\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  RAVDESS: Acc=0.1486, F1=0.0385, Loss=3.1520\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  TESS: Acc=0.1587, F1=0.0494, Loss=3.4297\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  SUBESCO: Acc=0.1759, F1=0.0541, Loss=2.2326\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-867353192.py:1126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ShEMO: Acc=0.3685, F1=0.2048, Loss=1.4333\n",
            "\n",
            "  Global Audio Avg: Acc=0.2234, F1=0.0959, Loss=2.4806\n",
            "\n",
            "================================================================================\n",
            "ROUND 2/30\n",
            "================================================================================\n",
            "LR multiplier: 1.0000\n",
            "\n",
            "--- TEXT MODALITY ---\n",
            "\n",
            "Training text client: dair-ai/emotion\n",
            "  Epoch 1: Loss=0.3837, Acc=0.9104\n",
            "  Epoch 2: Loss=0.1389, Acc=0.9616\n",
            "  Epoch 3: Loss=0.0911, Acc=0.9736\n",
            "\n",
            "Training text client: boltuix/emotions-dataset\n",
            "  Epoch 1: Loss=1.1706, Acc=0.6102\n",
            "  Epoch 2: Loss=0.8083, Acc=0.7119\n",
            "  Epoch 3: Loss=0.6485, Acc=0.7620\n",
            "\n",
            "Training text client: mteb/emotion\n",
            "  Epoch 1: Loss=0.3812, Acc=0.9121\n",
            "  Epoch 2: Loss=0.1405, Acc=0.9593\n",
            "  Epoch 3: Loss=0.0936, Acc=0.9709\n",
            "\n",
            "Training text client: go_emotions\n",
            "  Epoch 1: Loss=1.0411, Acc=0.6036\n",
            "  Epoch 2: Loss=0.6288, Acc=0.7729\n",
            "  Epoch 3: Loss=0.4961, Acc=0.8164\n",
            "\n",
            "Training text client: pashupatigupta/emotion\n",
            "  Epoch 1: Loss=1.5198, Acc=0.3721\n",
            "  Epoch 2: Loss=1.3650, Acc=0.4309\n",
            "  Epoch 3: Loss=1.2416, Acc=0.4853\n",
            "\n",
            "Aggregating text models...\n",
            "\n",
            "Text validation results:\n",
            "  dair-ai/emotion: Acc=0.9237, F1=0.8911, Loss=0.2774\n",
            "  boltuix/emotions-dataset: Acc=0.5510, F1=0.3833, Loss=2.4525\n",
            "  mteb/emotion: Acc=0.9270, F1=0.9040, Loss=0.2608\n",
            "  go_emotions: Acc=0.2533, F1=0.1655, Loss=2.6732\n",
            "  pashupatigupta/emotion: Acc=0.3010, F1=0.2471, Loss=2.2811\n",
            "\n",
            "  Global Text Avg: Acc=0.5912, F1=0.5182, Loss=1.5890\n",
            "\n",
            "--- AUDIO MODALITY ---\n",
            "\n",
            "Training audio client: CREMA-D\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1: Loss=62.6772, Acc=0.2107, F1=0.1129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2: Loss=393.2032, Acc=0.2106, F1=0.1105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3: Loss=1050.5714, Acc=0.2160, F1=0.1159\n",
            "\n",
            "Training audio client: RAVDESS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1: Loss=16.4206, Acc=0.1401, F1=0.0733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2: Loss=70.1221, Acc=0.1338, F1=0.0392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3: Loss=170.7522, Acc=0.1507, F1=0.0778\n",
            "\n",
            "Training audio client: TESS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1: Loss=35.1719, Acc=0.1424, F1=0.0618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2: Loss=89.6600, Acc=0.1481, F1=0.0557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3: Loss=189.7083, Acc=0.1555, F1=0.0617\n",
            "\n",
            "Training audio client: SUBESCO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1: Loss=50.1514, Acc=0.2240, F1=0.1257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2: Loss=274.3457, Acc=0.2657, F1=0.1386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3: Loss=696.9549, Acc=0.2622, F1=0.1375\n",
            "\n",
            "Training audio client: ShEMO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1: Loss=16.3443, Acc=0.3813, F1=0.3242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2: Loss=72.7662, Acc=0.4358, F1=0.3770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1041: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3: Loss=178.5129, Acc=0.4813, F1=0.4211\n",
            "\n",
            "Aggregating audio models...\n",
            "\n",
            "Audio validation results:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CREMA-D: Acc=0.2808, F1=0.1460, Loss=2.1390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  RAVDESS: Acc=0.1988, F1=0.1153, Loss=3.1695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  TESS: Acc=0.1964, F1=0.0995, Loss=3.2935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  SUBESCO: Acc=0.1889, F1=0.0784, Loss=2.0640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-867353192.py:1126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ShEMO: Acc=0.4759, F1=0.3865, Loss=1.4483\n",
            "\n",
            "  Global Audio Avg: Acc=0.2682, F1=0.1652, Loss=2.4229\n",
            "\n",
            "================================================================================\n",
            "ROUND 3/30\n",
            "================================================================================\n",
            "LR multiplier: 1.0000\n",
            "\n",
            "--- TEXT MODALITY ---\n",
            "\n",
            "Training text client: dair-ai/emotion\n",
            "  Epoch 1: Loss=0.1789, Acc=0.9526\n",
            "  Epoch 2: Loss=0.1028, Acc=0.9687\n",
            "  Epoch 3: Loss=0.0733, Acc=0.9777\n",
            "\n",
            "Training text client: boltuix/emotions-dataset\n",
            "  Epoch 1: Loss=1.0249, Acc=0.6623\n",
            "  Epoch 2: Loss=0.6870, Acc=0.7471\n",
            "  Epoch 3: Loss=0.5292, Acc=0.7974\n",
            "\n",
            "Training text client: mteb/emotion\n",
            "  Epoch 1: Loss=0.1875, Acc=0.9486\n",
            "  Epoch 2: Loss=0.1061, Acc=0.9679\n",
            "  Epoch 3: Loss=0.0685, Acc=0.9798\n",
            "\n",
            "Training text client: go_emotions\n",
            "  Epoch 1: Loss=0.9565, Acc=0.6429\n",
            "  Epoch 2: Loss=0.5674, Acc=0.7878\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-867353192.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOCAL_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 loss, acc = train_text_model_one_epoch(\n\u001b[0m\u001b[1;32m   1296\u001b[0m                     \u001b[0mlocal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_client_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-867353192.py\u001b[0m in \u001b[0;36mtrain_text_model_one_epoch\u001b[0;34m(model, loader, optimizer, scheduler, criterion)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRAD_CLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import uuid\n",
        "import faiss\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import google.generativeai as genai\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer, AutoModel, Wav2Vec2Processor, Wav2Vec2Model\n",
        "from fastapi import FastAPI, File, UploadFile, Form\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\"\"\"\n",
        "Trained model .pth must be saved in\n",
        "saved_models/text_emotion_model.pth\n",
        "saved_models/audio_emotion_model.pth\n",
        "\"\"\"\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=\"AIzaSyBVySqAtcZTq5XjLdnV1ryitgCluqvHzts\")\n",
        "\n",
        "# Paths\n",
        "UPLOAD_DIR = \"uploads\"\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "curriculum_graph = nx.Graph()\n",
        "\n",
        "class TextEmotionModel(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased\", num_labels=6):\n",
        "        super(TextEmotionModel, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.classifier = torch.nn.Linear(self.model.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "class AudioEmotionModel(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"facebook/wav2vec2-base\", num_labels=6):\n",
        "        super(AudioEmotionModel, self).__init__()\n",
        "        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "        self.model = Wav2Vec2Model.from_pretrained(model_name)\n",
        "        self.classifier = torch.nn.Linear(self.model.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        outputs = self.model(input_values)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        pooled_output = torch.mean(hidden_states, dim=1)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "def load_text_model(path):\n",
        "    model = TextEmotionModel()\n",
        "    model.load_state_dict(torch.load(path, map_location=torch.device(\"cpu\")))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def load_audio_model(path):\n",
        "    model = AudioEmotionModel()\n",
        "    model.load_state_dict(torch.load(path, map_location=torch.device(\"cpu\")))\n",
        "    model.eval()\n",
        "    return model\n",
        "def predict_text(model, text):\n",
        "    tokenizer = model.tokenizer\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    emotion_id = torch.argmax(logits, dim=1).item()\n",
        "    emotions = [\"happy\", \"sad\", \"angry\", \"neutral\", \"confused\", \"excited\"]\n",
        "    return emotions[emotion_id]\n",
        "\n",
        "def predict_audio(model, file_path):\n",
        "    processor = model.processor\n",
        "    speech, sr = librosa.load(file_path, sr=16000)\n",
        "    inputs = processor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs[\"input_values\"])\n",
        "    emotion_id = torch.argmax(logits, dim=1).item()\n",
        "    emotions = [\"happy\", \"sad\", \"angry\", \"neutral\", \"confused\", \"excited\"]\n",
        "    return emotions[emotion_id]\n",
        "curriculum_text = \"\"\n",
        "study_materials_texts = []\n",
        "study_materials_embeddings = []\n",
        "topics_graph = nx.Graph()\n",
        "roadmap = []\n",
        "\n",
        "# ==== Load your trained emotion models ====\n",
        "text_model_path = \"saved_models/text_emotion_model.pth\"   # update if needed\n",
        "audio_model_path = \"saved_models/audio_emotion_model.pth\" # update if needed\n",
        "\n",
        "text_model = load_text_model(text_model_path)\n",
        "audio_model = load_audio_model(audio_model_path)\n",
        "\n",
        "print(\"✅ Emotion models loaded successfully!\")\n",
        "\n",
        "# Load embedding model\n",
        "embedder_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedder_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def embed_text(text):\n",
        "    inputs = embedder_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        embeddings = embedder_model(**inputs).last_hidden_state.mean(dim=1)\n",
        "    return embeddings[0].numpy()\n",
        "\n",
        "dimension = 384\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "@app.post(\"/upload_curriculum/\")\n",
        "async def upload_curriculum(file: UploadFile = File(...)):\n",
        "    global curriculum_text\n",
        "    file_path = os.path.join(UPLOAD_DIR, file.filename)\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "    curriculum_text = extract_text(file_path)\n",
        "    return {\"message\": \"Curriculum uploaded and processed.\"}\n",
        "from fastapi import HTTPException\n",
        "from typing import Optional\n",
        "\n",
        "# helper: extract text from pdf (used earlier)\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def extract_text(path: str) -> str:\n",
        "    reader = PdfReader(path)\n",
        "    out = []\n",
        "    for p in reader.pages:\n",
        "        t = p.extract_text()\n",
        "        if t:\n",
        "            out.append(t)\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "# chunk helper (split large context for both embeddings and for sending to LLM)\n",
        "def chunk_text_for_index(text: str, max_chars: int = 1000):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), max_chars):\n",
        "        chunks.append(text[i:i+max_chars])\n",
        "    return chunks\n",
        "\n",
        "# store mapping from index -> chunk_text\n",
        "# (If you used study_materials_texts earlier, ensure it's the same list; we'll use `study_materials_texts`)\n",
        "# study_materials_texts and index were defined in PART 3\n",
        "\n",
        "@app.post(\"/upload_study_material/\")\n",
        "async def upload_study_material(file: UploadFile = File(...)):\n",
        "    \"\"\"\n",
        "    Accept a single PDF (or text) file, extract chunks, embed them, and add to FAISS index.\n",
        "    \"\"\"\n",
        "    global study_materials_texts, index\n",
        "\n",
        "    file_path = os.path.join(UPLOAD_DIR, file.filename)\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "\n",
        "    text = extract_text(file_path)\n",
        "    chunks = chunk_text_for_index(text, max_chars=800)\n",
        "\n",
        "    for ch in chunks:\n",
        "        emb = embed_text(ch)\n",
        "        emb = emb.astype(\"float32\")\n",
        "        index.add(np.expand_dims(emb, axis=0))  # add to faiss\n",
        "        study_materials_texts.append(ch)\n",
        "\n",
        "    return {\"message\": \"Study material uploaded\", \"chunks_added\": len(chunks)}\n",
        "\n",
        "\n",
        "@app.post(\"/ask_question/\")\n",
        "async def ask_question(\n",
        "    question: Optional[str] = Form(None),\n",
        "    audio: Optional[UploadFile] = File(None),\n",
        "    top_k: int = Form(3)\n",
        "):\n",
        "    \"\"\"\n",
        "    Accepts text (question) OR audio file (wav/mp3). Predicts emotion and answers using Gemini.\n",
        "    Provide question if you want semantic search context; audio-only allowed (question can be empty).\n",
        "    \"\"\"\n",
        "    # --- check data ---\n",
        "    if not question and not audio:\n",
        "        raise HTTPException(status_code=400, detail=\"Provide either 'question' text or an 'audio' file.\")\n",
        "\n",
        "    # --- Step 1: emotion detection ---\n",
        "    detected_emotion = \"neutral\"\n",
        "    confidence = None\n",
        "    try:\n",
        "        if audio:\n",
        "            # save temp audio\n",
        "            temp_path = os.path.join(UPLOAD_DIR, f\"temp_{uuid.uuid4().hex}_{audio.filename}\")\n",
        "            with open(temp_path, \"wb\") as f:\n",
        "                f.write(await audio.read())\n",
        "            # predict_audio returns emotion (and optionally confidence in your implementation)\n",
        "            # Here we assume predict_audio(model, path) -> emotion (string) OR (emotion, conf, probs)\n",
        "            try:\n",
        "                # attempt tuple signature like earlier notebook (emotion, confidence, probs)\n",
        "                res = predict_audio(audio_model, temp_path)\n",
        "                if isinstance(res, tuple):\n",
        "                    detected_emotion, confidence, _ = res\n",
        "                else:\n",
        "                    detected_emotion = res\n",
        "            except Exception:\n",
        "                # fallback to simple function (the lighter variant in PART 2)\n",
        "                detected_emotion = predict_audio(audio_model, temp_path)\n",
        "            # remove temp file\n",
        "            try:\n",
        "                os.remove(temp_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        elif question:\n",
        "            try:\n",
        "                res = predict_text(text_model, question)\n",
        "                # If your predict_text returns tuple (emotion, conf, probs) as in the long loader,\n",
        "                # handle that; otherwise assume it returns emotion string.\n",
        "                if isinstance(res, tuple):\n",
        "                    detected_emotion, confidence, _ = res\n",
        "                else:\n",
        "                    detected_emotion = res\n",
        "            except Exception:\n",
        "                detected_emotion = predict_text(text_model, question)\n",
        "    except Exception as e:\n",
        "        # ignore model failure but log\n",
        "        print(\"Emotion detection failed:\", e)\n",
        "        detected_emotion = \"neutral\"\n",
        "\n",
        "    # --- Step 2: semantic retrieval (if question present) ---\n",
        "    retrieved_context = \"\"\n",
        "    if question and len(study_materials_texts) > 0 and index.ntotal > 0:\n",
        "        q_emb = embed_text(question).astype(\"float32\")\n",
        "        q_emb = np.expand_dims(q_emb, axis=0)\n",
        "        k = min(top_k, index.ntotal)\n",
        "        D, I = index.search(q_emb, k)\n",
        "        hits = [study_materials_texts[idx] for idx in I[0] if idx < len(study_materials_texts)]\n",
        "        retrieved_context = \"\\n\\n\".join(hits)\n",
        "    elif not question:\n",
        "        retrieved_context = \"\"  # question may be spoken — in that case, we could transcribe audio externally\n",
        "\n",
        "    # --- Step 3: curriculum roadmap context ---\n",
        "    # build a readable roadmap string from the topics_graph or the curriculum you built earlier\n",
        "    # For simplicity, if you maintained a `roadmap` list from earlier, convert to text.\n",
        "    roadmap_text = \"\"\n",
        "    try:\n",
        "        # If curriculum graph exists and has method generate_graph_roadmap\n",
        "        if hasattr(curriculum_graph, \"generate_graph_roadmap\"):\n",
        "            rm = curriculum_graph.generate_graph_roadmap()\n",
        "            lines = []\n",
        "            for lvl, topics in rm.items():\n",
        "                lines.append(f\"{lvl}: {', '.join(topics)}\")\n",
        "            roadmap_text = \"\\n\".join(lines)\n",
        "    except Exception:\n",
        "        roadmap_text = \"\"\n",
        "\n",
        "    # --- Step 4: build emotion-aware prompt ---\n",
        "    emotion_instruction_map = {\n",
        "        \"happy\": \"Be encouraging and build on the user's enthusiasm.\",\n",
        "        \"sad\": \"Be gentle, supportive, and concise. Offer small, clear steps.\",\n",
        "        \"angry\": \"Be calm, clear, and avoid confrontational phrasing.\",\n",
        "        \"confused\": \"Use simple examples, step-by-step explanations, and check understanding.\",\n",
        "        \"excited\": \"Be encouraging and give engaging examples.\",\n",
        "        \"neutral\": \"Be clear, concise, and instructional.\"\n",
        "    }\n",
        "    tone_instr = emotion_instruction_map.get(detected_emotion.lower(), emotion_instruction_map[\"neutral\"])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an educational assistant and tutor. The student currently feels: {detected_emotion}.\n",
        "Tone instruction: {tone_instr}\n",
        "\n",
        "Curriculum Roadmap:\n",
        "{roadmap_text}\n",
        "\n",
        "Relevant Study Material (from uploaded files):\n",
        "{retrieved_context}\n",
        "\n",
        "Question:\n",
        "{question or '[no explicit text question; audio used]'}\n",
        "\"\"\"\n",
        "\n",
        "    # --- Step 5: call Gemini ---\n",
        "    try:\n",
        "        response = genai.GenerativeModel(\"models/gemini-2.5-flash\").generate_content(\n",
        "            prompt,\n",
        "            generation_config={\"temperature\": 0.2, \"max_output_tokens\": 500}\n",
        "        )\n",
        "        answer_text = response.text or \"\"\n",
        "    except Exception as e:\n",
        "        print(\"Gemini call failed:\", e)\n",
        "        answer_text = \"Sorry — failed to generate answer right now.\"\n",
        "\n",
        "    return {\n",
        "        \"emotion_detected\": detected_emotion,\n",
        "        \"confidence\": confidence,\n",
        "        \"answer\": answer_text\n",
        "    }\n",
        "@app.get(\"/topics/\")\n",
        "def list_topics():\n",
        "    # If you built topics into a graph or list, return them. Example:\n",
        "    try:\n",
        "        return {\"total_topics\": len(curriculum_graph.get_all_topics()), \"topics\": curriculum_graph.get_all_topics()}\n",
        "    except Exception:\n",
        "        return {\"total_topics\": 0, \"topics\": []}\n",
        "\n",
        "\n",
        "@app.get(\"/roadmap/\")\n",
        "def get_roadmap():\n",
        "    try:\n",
        "        rm = curriculum_graph.generate_graph_roadmap()\n",
        "        return {\"roadmap\": rm}\n",
        "    except Exception:\n",
        "        return {\"roadmap\": {}}\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"message\": \"Emotion-aware Curriculum QA running.\"}\n"
      ],
      "metadata": {
        "id": "A7QRnG4u45rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EVkaD1WC6JUC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}